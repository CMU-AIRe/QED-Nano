<div align="center">
    <h1><img height="150px" src="./images/logo.png" alt="QED Nano"><br>QED Nano</h1>

  <a href="https://www.python.org/">
<img alt="Build" src="https://img.shields.io/badge/Python-3.12-1f425f.svg?color=blue">
  </a>
  <a href="https://opensource.org/licenses/MIT">
<img alt="License: MIT" src="https://img.shields.io/badge/License-MIT-green.svg">
  </a>
  <a href="https://huggingface.co/lm-provers">
<img alt="Datasets" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-lm--prover-ffc107?color=ffc107&logoColor=white">
  </a>
</div>

# QED-Nano: Nearing Gemini 3 Pro on Olympiad Math Proofs with a 4B Model

This repository contains the code and instructions associated with the QED Nano model, a 4B-parameter language model fine-tuned to solve mathematical proof-based problems. QED Nano is close to Google's Gemini 3 Pro on the IMOProofBench, and outperforms GPT-OSS-120B. Our instructions are divided in two parts: (1) how to our evaluation benchmarks, and (2) how to train your own model using our codebase and datasets.

## Installation
This repository uses uv for environment and dependency management. To set up the environment, run:
```shell
uv venv .venv --python 3.12 && source .venv/bin/activate
uv pip install .
```

## Benchmarking

### Running Models
To run a new model on any benchmark you should first create a config file for it in `configs/models/`. You can use the existing configs as examples. Many API providers are already set up, but you can also add new by following [this example](configs/models/openai/custom-oss-120b.yaml). New APIs are assumed to be compatible with the OpenAI completions API. The default supported APIs are:
- openai: Set OPENAI_API_KEY in your environment.
- google: Set GOOGLE_API_KEY in your environment.
- together: Set TOGETHER_API_KEY in your environment.
- openrouter: Set OPENROUTER_API_KEY in your environment.

Our framework support three separate benchmarks: IMOProofBench, IMOAnswerBench, and ProofBench. To run any benchmark, use:
```bash
# For IMOProofBench
uv run python scripts/run.py --model-config google/gemini-3-pro --output-path outputs/gemini.jsonl
# For IMOAnswerBench
uv run python scripts/run.py --model-config google/gemini-3-pro --output-path outputs/gemini.jsonl --final-answer
# For ProofBench
uv run python scripts/run.py --model-config google/gemini-3-pro --data-path lm-provers/ProofBench --output-path outputs/gemini.jsonl
```
The model config should be specified relative to `configs/models/` (e.g., `openai/gemini`). By default, intermediate results will be reused, unless you specify `--overwrite`. You can also run multiple times over the dataset using the `--n` flag. The output will be saved to `outputs/gemini.jsonl`.  The script will store the same columns as the input dataset, with additional columns:
- `model_solution`: The model's response.
- `cost_run`: The cost incurred during generation. Dictionary consisting of `input_tokens`, `output_tokens`, and `cost` (latter is in USD).
- `history`: The full interaction history with the model.

### Grading Model Runs
Grading can take place on basically any dataset that has the following fields:
- `problem`: The problem statement.
- `model_solution`: The solution generated by a model.
- `grading_guidelines`: The grading guidelines of the problem as a string. Only necessary for proof-based benchmarks like IMOProofBench/ProofBench.
- `answer`: The correct answer to the problem. Only necessary for final-answer benchmarks like IMOAnswerBench.
- `solution`: The ground-truth solution to the problem. Only necessary for proof-based benchmarks like IMOProofBench/ProofBench.

To run grading, use:
```bash
# For IMOProofBench
uv run python scripts/eval.py --model-config google/gemini-3-pro --data-path outputs/gemini.jsonl
# For IMOAnswerBench
uv run python scripts/eval.py --model-config google/gemini-3-pro --data-path outputs/gemini.jsonl --final-answer
# For ProofBench
uv run python scripts/eval.py --model-config google/gemini-3-pro --data-path outputs/gemini.jsonl --proofbench
```
The model config now indicates the grader model to use. By default, intermediate results will be reused, unless you specify `--overwrite`. The script will store the result in the same dataset as the input with additional columns:
- `grade`: The grade assigned by the model, as a list of dicts with `points` and `desc` keys.
- `schema_0`: The grading schema used, as a list of dicts with `title`, `desc`, and `points` keys.
- `grade_cost`: The cost incurred during grading. Dictionary consisting of `input_tokens`, `output_tokens`, and `cost` (latter is in USD).
- `score`: The overall score of the model. Either 0 or 1 for IMOAnswerBench, or the total points assigned (out of 7) for IMOProofBench/ProofBench.
Both `grade` and `schema_0` are lists of dicts to allow for compatibility with our visualization tool.

### Retrieve Results
You can compute the results using the `scripts/stats.py` script. For example:

```bash
uv run python scripts/stats.py outputs/gemini.jsonl
```

### Agentic Evaluation
Our code additionally enables the use of several agentic frameworks. To run an agent, first create a config under `configs/models` that matches an agent scaffold under `configs/agents` with a specific model. For an example, see [this config](configs/models/agent/nano-deepseek.yaml) which uses the `deepseek_math` scaffold. To run this config, follow the same instructions as above, but specify the agent config instead of a model config.

The following agents are supported:
- `deepseek_math`: The DeepSeekMath agent from [DeepSeek's paper](https://arxiv.org/abs/2511.22570). In our experiments, this agent performed the best.
- `nomos`: The Nomos agent from [Nomos' release](https://github.com/NousResearch/nomos).
- `reasoning_cache`: The Reasoning Cache agent from [Reasoning Cache's paper](https://arxiv.org/abs/2602.03773).
- `rsa`: The RSA agent from [this paper](https://arxiv.org/abs/2509.26626).
- `selfcheck`: A simple generate-then-verify loop from [this paper](https://arxiv.org/abs/2507.15855).
- `math_agent` and `static_math_agent`: Two simple agentic frameworks we implemented ourselves. The former allows for dynamic tool use, planner agent style: the agent can call itself as LLM with specific prompts that solve, verify, check, merge, ... the solutions. The latter is a more static framework where all steps are predefined. The dynamic framework did not perform well in our experiments (models struggled to learn the tool use), but the static framework performed decently, so we include both for completeness.

### Additional Scripts
We also provide several additional scripts for reproducing some of the results described in our blog post:

- **[`scheme_generation.py`](scripts/scheme_generation.py)** <br>Script to generate grading schemes for problems in our training data. The script assumes that the input `data-path` has `problem` and `solution` columns, and it will create a new dataset with an additional `grading_scheme` column containing the generated grading scheme at `oututs/grading_schemes.jsonl` by default. The prompt used for grading scheme generation is in [this file](`configs/prompts/schema_generation.txt`), and the model can be specified using `--model-config` as usual.
- **[`filtering.py`](scripts/filtering.py)**<br>Script to filter the training data based on various criteria by performing LLM-based evaluation. It assumes that the input `data-path` has the same columns as [AI-MO/aops_raw](https://huggingface.co/datasets/AI-MO/aops_raw) and it will create a new dataset with additional columns containing the results of the filtering at `outputs/filtered_data.jsonl` by default. The prompt used for filtering is in [this file](`configs/prompts/filtering.txt`), and the model can be specified using `--model-config` as usual.
- **[`grading.py`](scripts/grading.py)**<br>We used this script to perform our experiments with different grading models and prompts. To run this script on our benchmarks, use:
   ```bash
   # For the MathArena subset
   uv run python scripts/grading.py --model-config google/gemini-3-pro --prompt {path_to_prompt} --matharena --output-path outputs/grading_results.jsonl
   # For the in-distribution prompt-set
   uv run python scripts/grading.py --model-config google/gemini-3-pro --prompt {path_to_prompt} --output-path outputs/grading_results.jsonl
   ```
   Additionally, specify the `--ground-truth` flag to use the grading prompt that includes the ground-truth solution (this option is not available for the MathArena subset). You can then use the `stats.py` script to compute the results:
   ```bash
   uv run python scripts/stats.py outputs/grading_results.jsonl --grader
   ```

### Visualization Tool
We provide a visualization tool for exploring the results of model runs and grading. To run the tool, follow the instructions in [app/README.md](app/README.md).

### SLURM Scripts
Quick example:
```bash
# From repo root - runs inference + evaluation automatically
# Note: API judge configs live under configs/models/<provider>/ (e.g., google/gemini-3-pro).
# queue_launch_imobench.sh expects the provider subdir path (google/gemini-3-pro)
bash slurm/queue_launch_imobench.sh \
  --model Qwen/Qwen3-32B \
  --judge google/gemini-3-pro
```

See [slurm/README.md](../slurm/README.md) for complete documentation.
